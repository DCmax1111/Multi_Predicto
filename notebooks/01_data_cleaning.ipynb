{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laptop Price Prediction - Data Cleaning\n",
    "This notebook covers all steps to clean and prepare the dataset for modeling.\n",
    "We will:\n",
    "1. Drop unnecessary columns.\n",
    "2. Convert text columns to numeric.\n",
    "3. Parse complex columns (Memory, ScreenResolution).\n",
    "4. Encode categorical features.\n",
    "5. Prepare the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import, Load & Preview\n",
    "- Import `pandas` and `numpy` as pd and np respectively.\n",
    "- Load the dataset.\n",
    "- Display the column names to get a good overview of everything.\n",
    "- Display the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01_data_cleaning.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\"../data/raw/laptop_price.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the column names\n",
    "print(\"Columns in dataset:\")\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Drop Unnecesary Column(s)\n",
    "We drop `laptop_ID` column because it doesn't provide useful information for predicting the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.notnull().sum()\n",
    "# data.head().T\n",
    "print(len(data['Product'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "data = data.drop(['laptop_ID'], axis=1)\n",
    "# Dropping laptop_ID because it's similar to the index.\n",
    "print(data.columns.tolist())\n",
    "# data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Convert RAM To Numeric\n",
    "The `Ram` column contains text like \"8GB\". We remove \"GB\" and convert it to integer for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Ram'] = data['Ram'].str.replace('GB', '').astype(int)\n",
    "data['Ram'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert Weight To Numeric\n",
    "The `Weight` column contains text like \"1.37kg\". We remove \"kg\" and convert it to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Weight'] = data['Weight'].str.replace('kg', '').astype(float)\n",
    "data['Weight'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Parse Memory Column\n",
    "The `Memory` column contains text like \"256GB SSD + 1TB HDD\".\n",
    "We will split it into separate columns for SSD, HDD, Hybrid storage and convert everything to GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns with default 0\n",
    "data['SSD'] = 0\n",
    "data['HDD'] = 0\n",
    "data['Hybrid'] = 0\n",
    "data['Flash_Storage'] = 0\n",
    "\n",
    "import re\n",
    "\n",
    "# Function to convert memory strings to numbers.\n",
    "def convert_memory(mem):\n",
    "    mem = str(mem)\n",
    "    ssd = hdd = hybrid = flash = 0  # Start with 0 for all storage types\n",
    "\n",
    "    # Split by '+'\n",
    "    parts = mem.split('+')\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "\n",
    "        # Extract numeric size\n",
    "        size_match = re.search(r'(\\d+)', part)\n",
    "        size = int(size_match.group(1)) if size_match else 0\n",
    "\n",
    "        # Convert TB â†’ GB\n",
    "        if \"TB\" in part:\n",
    "            size *= 1024\n",
    "\n",
    "        # Assign to storage type\n",
    "        if \"SSD\" in part:\n",
    "            ssd += size\n",
    "        elif \"HDD\" in part:\n",
    "            hdd += size\n",
    "        elif \"Hybrid\" in part:\n",
    "            hybrid += size\n",
    "        elif \"Flash\" in part or \"Flash Storage\" in part:\n",
    "            flash += size\n",
    "\n",
    "    return pd.Series([ssd, hdd, hybrid, flash])\n",
    "\n",
    "# Apply function\n",
    "data[['SSD', 'HDD', 'Hybrid', 'Flash_Storage']] = data['Memory'].apply(convert_memory)\n",
    "data = data.drop('Memory', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Parse ScreenResolution\n",
    "We will extract:\n",
    "1. X_resolution\n",
    "2. Y_resolution\n",
    "3. Touchscreen (if mentioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['ScreenResolution'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Touchscreen column\n",
    "data['Touchscreen'] = data['ScreenResolution'].apply(lambda x: 1 if 'Touchscreen' in x else 0)\n",
    "\n",
    "# Extract X and Y resolution\n",
    "data['X_res'] = data['ScreenResolution'].str.split('x').str[0].str.extract(\"(\\d+)\").astype(int)\n",
    "data['Y_res'] = data['ScreenResolution'].str.split('x').str[1].str.extract(\"(\\d+)\").astype(int)\n",
    "\n",
    "# Drop original ScreenResolution column\n",
    "data = data.drop('ScreenResolution', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Simplify CPU & GPU\n",
    "We will only keep the CPU and GPU brand names for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU brand\n",
    "data['Cpu_brand'] = data['Cpu'].apply(lambda x: x.split()[0])\n",
    "data = data.drop('Cpu', axis=1)\n",
    "\n",
    "# GPU brand\n",
    "data['Gpu_brand'] = data['Gpu'].apply(lambda x: x.split()[0])\n",
    "data = data.drop('Gpu', axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Encode Product Column\n",
    "The `Product` column contains 618 unique values, which makes one-hot encoding impractical because it would create hundreds of new columns.\n",
    "to handle this efficiently, we will use **Hashtag Encoding**, which maps each product into a fixed number of numeric columns (hash buckets).\n",
    "\n",
    "This reduces dimensionality while still capturing useful patterns from the `Product` names. We will start with 10 hash components, but this number can be tuned (e.g., 5, 10, 20) to balance performance and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.get_dummies(data, columns=['TypeName', 'Cpu_brand', 'Gpu_brand', 'OpSys'], drop_first=False)\n",
    "from category_encoders import HashingEncoder\n",
    "\n",
    "# initialize encoder with 10 hash components\n",
    "encoder = HashingEncoder(cols=['Product'], n_components=10)\n",
    "\n",
    "# Fit and transform the dataset\n",
    "data = encoder.fit_transform(data)\n",
    "\n",
    "# The `Product` column is automatically replaced with hashed numeric features.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Prepare Target Variable\n",
    "Ensure Price_euros is numeric and check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Price_euros'] = pd.to_numeric(data['Price_euros'], errors='coerce')\n",
    "data = data.dropna()  # Drop rows with missing values\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Save Cleaned Dataset\n",
    "We save the cleaned dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/processed/laptops_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
